{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a388b5",
   "metadata": {},
   "source": [
    "20260211\n",
    "recording some nlp tricks\n",
    "- original blog: https://machinelearningmastery.com/7-advanced-feature-engineering-tricks-using-llm-embeddings/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f53ba",
   "metadata": {},
   "source": [
    "# Semantic Similarity as a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96e98f",
   "metadata": {},
   "source": [
    "use anchor in sentence similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ac66ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilun.zhang/VSCodeProjects/aie/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1104.34it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36162615 0.5896069  0.03221758]]\n"
     ]
    }
   ],
   "source": [
    "# don't know, should always use: ./.venv//bin/python -m pip install sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    " \n",
    "# Initialize model and encode anchors\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "anchors = [\"billing issue\", \"login problem\", \"feature request\"]\n",
    "anchor_embeddings = model.encode(anchors)\n",
    " \n",
    "# Encode a new ticket\n",
    "new_ticket = [\"I can't access my account, it says password invalid.\"]\n",
    "ticket_embedding = model.encode(new_ticket)\n",
    " \n",
    "# Calculate similarity features\n",
    "similarity_features = cosine_similarity(ticket_embedding, anchor_embeddings)\n",
    "print(similarity_features)  # e.g., [[0.1, 0.85, 0.3]] -> high similarity to \"login problem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a72b8f",
   "metadata": {},
   "source": [
    "try business case（used for text classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928872c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 975.10it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: shibing624/text2vec-base-chinese\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36438686 0.42217815]]\n"
     ]
    }
   ],
   "source": [
    "# don't know, should always use: ./.venv//bin/python -m pip install sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    " \n",
    "# Initialize model and encode anchors\n",
    "model = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
    "anchors = [\"买车\", \"没提到要买车\"]\n",
    "anchor_embeddings = model.encode(anchors)\n",
    " \n",
    "# Encode a new ticket\n",
    "new_ticket = [\"ET9展车已到门店，从兰州新区接送用户到万象城门店看车，单程60公里\"]\n",
    "ticket_embedding = model.encode(new_ticket)\n",
    " \n",
    "# Calculate similarity features\n",
    "similarity_features = cosine_similarity(ticket_embedding, anchor_embeddings)\n",
    "print(similarity_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1021.08it/s, Materializing param=pooler.dense.weight]                              \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: shibing624/text2vec-base-chinese\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5112165  0.5099434  0.42966738]]\n"
     ]
    }
   ],
   "source": [
    "# 测试：能否提取几种汽车品牌，不准。觉得是too specific。比如上面这种general带语义的判断结果还ok。\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    " \n",
    "# Initialize model and encode anchors\n",
    "model = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
    "anchors = [\"提到多个汽车品牌\", \"仅提到一种汽车品牌\", \"未提到汽车品牌\"]\n",
    "anchor_embeddings = model.encode(anchors)\n",
    " \n",
    "# Encode a new ticket\n",
    "new_ticket = [\"用户是经常在国外，但是一直有在炒股，比较关注蔚来，每次有新车型上市都会关心讨论。用户也接触过好几个顾问，但是觉得回复速度和专业性会差一些，比较认可我们。下单前，用户提出想要有人能接一下，跟淳淳约了et9体验，帮他协调，可以去接送他，觉得很尊贵，后面满意锁单\"]\n",
    "ticket_embedding = model.encode(new_ticket)\n",
    " \n",
    "# Calculate similarity features\n",
    "similarity_features = cosine_similarity(ticket_embedding, anchor_embeddings)\n",
    "print(similarity_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b75f0",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and Denoising\n",
    "LLM embeddings are high-dimensional (e.g., 384 or 768). Reducing dimensions can remove noise, cut computational cost, and sometimes reveal more accurate patterns.\n",
    "The “curse of dimensionality” means some models (like Random Forests) may perform poorly when many dimensions are uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf692a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (6, 768)\n",
      "Reduced shape: (6, 6)\n",
      "PCA retains 100.00% of variance.\n"
     ]
    }
   ],
   "source": [
    "text_dataset = [\n",
    "    \"I was charged twice for my subscription\",\n",
    "    \"Cannot reset my password\",\n",
    "    \"Would love to see dark mode added\",\n",
    "    \"My invoice shows the wrong amount\",\n",
    "    \"Login keeps failing with error 401\",\n",
    "    \"Please add export to PDF feature\",\n",
    "]\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE  # For visualization, not typically for feature engineering\n",
    " \n",
    "# Assume 'embeddings' is a numpy array of shape (n_samples, 384)\n",
    "embeddings = np.array([model.encode(text) for text in text_dataset])\n",
    " \n",
    "# Method 1: PCA - for linear relationships\n",
    "# n_components must be <= min(n_samples, n_features)\n",
    "n_components = min(50, len(text_dataset))\n",
    "pca = PCA(n_components=n_components)\n",
    "reduced_pca = pca.fit_transform(embeddings)\n",
    " \n",
    "# Method 2: TruncatedSVD - similar, works on matrices from TF-IDF as well\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "reduced_svd = svd.fit_transform(embeddings)\n",
    " \n",
    "print(f\"Original shape: {embeddings.shape}\")\n",
    "print(f\"Reduced shape: {reduced_pca.shape}\")\n",
    "print(f\"PCA retains {sum(pca.explained_variance_ratio_):.2%} of variance.\")\n",
    "\n",
    "# The code above works because PCA finds axes of maximum variance, often capturing the most significant semantic information in fewer, uncorrelated dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52269d",
   "metadata": {},
   "source": [
    "Note that dimensionality reduction is lossy. Always test whether reduced features maintain or improve model performance. PCA is linear; for nonlinear relationships, consider UMAP (but be mindful of its sensitivity to hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf3a71",
   "metadata": {},
   "source": [
    "# Cluster Labels and Distances as Features\n",
    "\n",
    "- Use unsupervised clustering on your collection embeddings to discover natural thematic groups. Use cluster assignments and distances to cluster centroids as new categorical and continuous features.\n",
    "- The problem: your data may have unknown or emerging categories not captured by predefined anchors (remember the semantic similarity trick). Clustering all document embeddings and then using the results as features addresses this.\n",
    "\n",
    "- (exactly what i did in my project...to check what texts are naturally together which will generate topics, which can be used to fill what we missed by using \"anchor list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# Cluster the embeddings\n",
    "# n_clusters must be <= n_samples\n",
    "n_clusters = min(10, len(text_dataset))\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    " \n",
    "# Feature 1: Cluster assignment (encode if your model needs numeric)\n",
    "encoder = LabelEncoder()\n",
    "cluster_id_feature = encoder.fit_transform(cluster_labels)\n",
    " \n",
    "# Feature 2: Distance to each cluster centroid\n",
    "distances_to_centroids = kmeans.transform(embeddings)  \n",
    "# Shape: (n_samples, n_clusters)\n",
    "# 'distances_to_centroids' now has up to n_clusters new continuous features per sample\n",
    " \n",
    "# Combine with original embeddings or use alone\n",
    "enhanced_features = np.hstack([embeddings, distances_to_centroids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a26227",
   "metadata": {},
   "source": [
    "- This works because it provides the model with structural knowledge about the data’s natural grouping, which can be highly informative for tasks like classification or anomaly detection.\n",
    "\n",
    "- Note: we’re using n_clusters = min(10, len(text_dataset)) because we don’t have much data. Choosing the number of clusters (k) is critical—use the elbow method or domain knowledge. DBSCAN is an alternative for density-based clustering that does not require specifying k.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5e11b",
   "metadata": {},
   "source": [
    "# Text Difference Embeddings\n",
    "- For tasks involving pairs of texts (for example, duplicate-question detection and semantic search relevance), the interaction between embeddings is more important than the embeddings in isolation.\n",
    "\n",
    "- Simply concatenating two embeddings doesn’t explicitly model their relationship. A better approach is to create features that encode the difference and element-wise product between embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdab717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3350369 , -0.1173774 ,  0.32774225, ...,  0.01259114,\n",
       "         1.8002882 ,  0.00377938]], shape=(1, 3072), dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For pairs of texts (e.g., query and document, ticket1 and ticket2)\n",
    "texts1 = [\"I can't log in to my account\"]\n",
    "texts2 = [\"Login keeps failing with error 401\"]\n",
    " \n",
    "embeddings1 = model.encode(texts1)\n",
    "embeddings2 = model.encode(texts2)\n",
    " \n",
    "# Basic concatenation (baseline)\n",
    "concatenated = np.hstack([embeddings1, embeddings2])\n",
    " \n",
    "# Advanced interaction features\n",
    "absolute_diff = np.abs(embeddings1 - embeddings2)  # Captures magnitude of disagreement\n",
    "elementwise_product = embeddings1 * embeddings2     # Captures alignment (like a dot product per dimension)\n",
    " \n",
    "# Combine all for a rich feature set\n",
    "interaction_features = np.hstack([embeddings1, embeddings2, absolute_diff, elementwise_product])\n",
    "interaction_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3403e",
   "metadata": {},
   "source": [
    "- Why does this work? The difference vector highlights where semantic meanings diverge. The product vector increases where they agree. This design is influenced by successful neural network architectures like Siamese Networks used in similarity learning.\n",
    "\n",
    "- This approach roughly quadruples the feature dimension. Apply dimensionality reduction (as above) and regularization to control size and noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d75d7",
   "metadata": {},
   "source": [
    "#  Sentence-Level vs. Word-Level Embedding Aggregation\n",
    "- The problem we're solving here: a single sentence embedding for a long, multi-topic document can lose fine-grained information.\n",
    "- LLMs can embed words, sentences, or paragraphs. For longer documents, strategically aggregating word-level embeddings can capture information that a single document-level embedding might miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a464277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To address this, use a token-embedding model (e.g., all-MiniLM-L6-v2 in word-piece mode or bert-base-uncased from Transformers), then pool key tokens.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    " \n",
    "# Load a model that provides token embeddings\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    " \n",
    "def get_pooled_embeddings(text, pooling_strategy=\"mean\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state  # Shape: (batch, seq_len, hidden_dim)\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)  # (batch, seq_len, 1)\n",
    " \n",
    "    if pooling_strategy == \"mean\":\n",
    "        # Masked mean to ignore padding tokens\n",
    "        masked = token_embeddings * attention_mask\n",
    "        summed = masked.sum(dim=1)\n",
    "        counts = attention_mask.sum(dim=1).clamp(min=1)\n",
    "        return (summed / counts).squeeze(0).numpy()\n",
    "    elif pooling_strategy == \"max\":\n",
    "        # Very negative number for masked positions\n",
    "        masked = token_embeddings.masked_fill(attention_mask == 0, -1e9)\n",
    "        return masked.max(dim=1).values.squeeze(0).numpy()\n",
    "    elif pooling_strategy == \"cls\":\n",
    "        return token_embeddings[:, 0, :].squeeze(0).numpy()\n",
    " \n",
    "# Example: Get mean of non-padding token embeddings\n",
    "doc_embedding = get_pooled_embeddings(\"A long document about several topics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25551d",
   "metadata": {},
   "source": [
    "- pooling\n",
    "    - Why it works: Mean pooling averages out noise, while max pooling highlights the most salient features. For tasks where specific keywords are critical (e.g., sentiment from “amazing” vs. “terrible”), this can be more effective than standard sentence embeddings.\n",
    "\n",
    "- padding & attention masks & CLS\n",
    "    - Note that this can be computationally heavier than sentence-transformers. It also requires careful handling of padding and attention masks. The [CLS] token embedding is often fine-tuned for specific tasks but may be less general as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f4855",
   "metadata": {},
   "source": [
    "# Embeddings as Input for Feature Synthesis (AutoML)\n",
    "- Let automated feature engineering tools treat your embeddings as raw input to discover complex, non-linear interactions you might not consider manually. Manually engineering interactions between hundreds of embedding dimensions is impractical.\n",
    "- One practical approach is to use scikit-learn’s PolynomialFeatures on reduced-dimension embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "# Start with reduced embeddings to avoid explosion\n",
    "# n_components must be <= min(n_samples, n_features)\n",
    "n_components_poly = min(20, len(text_dataset))\n",
    "pca = PCA(n_components=n_components_poly)\n",
    "embeddings_reduced = pca.fit_transform(embeddings)\n",
    " \n",
    "# Generate polynomial and interaction features up to degree 2\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "synthesized_features = poly.fit_transform(embeddings_reduced)\n",
    " \n",
    "print(f\"Reduced dimensions: {embeddings_reduced.shape[1]}\")\n",
    "print(f\"Synthesized features: {synthesized_features.shape[1]}\")  # n + n*(n+1)/2 features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

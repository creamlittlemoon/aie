{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966fcb7e",
   "metadata": {},
   "source": [
    "20260124\n",
    "\n",
    "- L1 (Foundation): clean Python, data handling, correctness, basic NLP\n",
    "- L2 (Core): REST services, async, evaluation, retrieval, model wrappers\n",
    "- L3 (Advanced): RAG pipelines, caching, observability, AWS patterns, CI/CD rigor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2ab1c",
   "metadata": {},
   "source": [
    "# A) python foundations for AI backends (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f21cb",
   "metadata": {},
   "source": [
    "## 1. re.sub()\n",
    "(whitespace, casing, punctuation)\n",
    "\n",
    "- Scenario: Chatbot preprocessor cleans user input.\n",
    "- Input: \"  Hi!!   I NEED   help...  \"\n",
    "- Output: \"hi! i need help...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e501b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!! i need help...\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def normalize(text:str) -> str:\n",
    "    text = text.strip().lower() # strip(): leading and trailing whitespace removed\n",
    "    text = re.sub(r\"\\s+\", \" \", text)    #collapses any run of whitespace(spaces, tabs, newlines) into a single space\n",
    "    text = re.sub(r\"!{2,}\", \"!\", text)  #collapses any run of 2 or more exclamation marks into a single !\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(normalize(\"  Hi!!   I NEED   help...  \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00349b2",
   "metadata": {},
   "source": [
    "## 2. parse_qsl: kv pairs\n",
    "Parse key=value pairs\n",
    "- Scenario: REST endpoint receives tracking params.\n",
    "- Input: \"lang=en&user=42&debug=true\"\n",
    "- Output: {\"lang\":\"en\",\"user\":\"42\",\"debug\":\"true\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7916561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lang': 'en', 'user': '42', 'debug': 'true'}\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import parse_qsl\n",
    "\n",
    "def  parse_query(q:str) -> dict:\n",
    "    return dict(parse_qsl(q, keep_blank_values=True))\n",
    "\n",
    "print(parse_query(\"lang=en&user=42&debug=true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff750a33",
   "metadata": {},
   "source": [
    "## 3. pathlib, Path\n",
    "Safe JSON read + validation fallback\n",
    "- Scenario: Load prompt templates from JSON; donâ€™t crash on bad file.\n",
    "- Input: bad JSON file\n",
    "- Output: default dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db62b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'You are helpful.'}\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from pathlib import Path \n",
    "\n",
    "def load_json(path: str, default: dict) -> dict:\n",
    "    try:\n",
    "        return json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "    except(OSError, json.JSONDecodeError):\n",
    "        return default \n",
    "\n",
    "cfg = load_json(\"prompts.json\", {\"system\": \"You are helpful.\"})\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73c028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: tmp/profile.json\n",
      "File contents:\n",
      " {\n",
      "  \"id\": 7,\n",
      "  \"tags\": [\n",
      "    \"new\",\n",
      "    \"vip\"\n",
      "  ],\n",
      "  \"active\": true\n",
      "}\n",
      "Loaded: {'id': 7, 'tags': ['new', 'vip'], 'active': True}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "user = {\"id\": 7, \"tags\": [\"new\", \"vip\"], \"active\": True}\n",
    "\n",
    "p = Path(\"tmp\") / \"profile.json\"\n",
    "p.parent.mkdir(exist_ok=True)\n",
    "\n",
    "p.write_text(json.dumps(user, indent=2))\n",
    "\n",
    "print(\"Saved to:\", p)\n",
    "print(\"File contents:\\n\", p.read_text())\n",
    "\n",
    "loaded = json.loads(p.read_text())\n",
    "print(\"Loaded:\", loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da68c3a",
   "metadata": {},
   "source": [
    "## 4. set() to deduplicate\n",
    "Deduplicate events while preserving order\n",
    "- Scenario: Avoid double-counting telemetry events.\n",
    "- Input: [\"open\",\"click\",\"open\",\"close\"]\n",
    "- Output: [\"open\",\"click\",\"close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7538458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['open', 'click', 'close']\n"
     ]
    }
   ],
   "source": [
    "from typing import List \n",
    "\n",
    "def dedupe_keep_order(items: List) -> List:\n",
    "    seen, out = set(), []   # ðŸ¤© Set for O(1), order preservation\n",
    "    for x in items:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out \n",
    "\n",
    "print(dedupe_keep_order([\"open\",\"click\",\"open\",\"close\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74d3513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "# key point: set() only contains unique values\n",
    "\n",
    "s = {1,1,2,3,3}\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ec2f4",
   "metadata": {},
   "source": [
    "## 5. @lru_cache\n",
    "\n",
    "Simple LRU cache for expensive operations\n",
    "- Scenario: cache embeddings or database lookups\n",
    "- input: repeated key\n",
    "- output: avoids recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadc343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_for_order_1\n",
      "result_for_order_1\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache # to memoize results of a function so repeated calls with the same input are fast. (LRU = least reacently used)\n",
    "\n",
    "@lru_cache(maxsize=1024)    #wraps the function with a cache that stores up to 1024 recent results.\n",
    "def expensive_lookup(key: str) -> str:\n",
    "    # pretend DB/ network call\n",
    "    return f\"result_for_{key}\"\n",
    "\n",
    "\n",
    "print(expensive_lookup(\"order_1\"))  #firstcall with \"order_1\" computes and stores the result\n",
    "print(expensive_lookup(\"order_1\"))  #cached (same key, so served from cache. no DB/network work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdab144",
   "metadata": {},
   "source": [
    "# B) Data processing with scientific stack (Numpy/pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afcaa2",
   "metadata": {},
   "source": [
    "## 6. pd.Series, s.str.len()\n",
    "Compute basic text stats with pandas\n",
    "- scenario: analyze chat logs\n",
    "- list of messages\n",
    "- avg length, empty rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2b387cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       hi\n",
      "1    need help with refund\n",
      "2                         \n",
      "3                   thanks\n",
      "dtype: str\n",
      "0     2\n",
      "1    21\n",
      "2     0\n",
      "3     6\n",
      "dtype: int64\n",
      "{'avg_len': np.float64(7.25), 'empty_rate': np.float64(0.25)}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "msgs = [\"hi\", \"need help with refund\", \"\", \"thanks\"]\n",
    "s = pd.Series(msgs) #create a 1d labeled array\n",
    "print(s)\n",
    "print(s.str.len())\n",
    "\n",
    "stats = {\n",
    "    \"avg_len\": s.str.len().mean(),  #adding \"float()\" also correct\n",
    "    \"empty_rate\":(s.str.len() == 0).mean()\n",
    "}\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5991aa",
   "metadata": {},
   "source": [
    "- don't know why my terminal venv is not the same as the one i use in ipynb, so...i tried to directly use :\n",
    "    - ./.venv/bin/python -m pip install pandas\n",
    "- to install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7bcae",
   "metadata": {},
   "source": [
    "## 7. pd.to_datetime(), datetime\n",
    "date parsing + timezone normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24T09:20:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ts = pd.to_datetime(\"2026-01-24T10:20:00+01:00\", utc=True)  #Parses the ISO string into a pandas Timestamp; utc=True converts the result to UTC time\n",
    "print(ts.isoformat())   # Prints the timestamp in ISO 8601 format (now in UTC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9cf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-24T09:20:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# can also use datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "ts = datetime.fromisoformat(\"2026-01-24T10:20:00+01:00\")    #parse ISo string into timezone-aware datetime object\n",
    "ts_utc = ts.astimezone(timezone.utc)    #since ts is now a datetime object. astimezone is a method of the OBJECT\n",
    "print(ts_utc.isoformat())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c31dc2",
   "metadata": {},
   "source": [
    "## 8. into batches (Iterable)\n",
    "- scenario: send documents to embedding api in batches\n",
    "- input: docs size=10, batch=4\n",
    "- output: 3 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14b62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c', 'd'], ['e', 'f', 'g', 'h'], ['i', 'j']]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "def batched(items: List[str], size:int) -> Iterable[List[str]]: #Iterable: a type hint for \"anything you can loop over\"(list, tuple, set, generator, etc.) so it can be List[List[str]], or Tuple(List[str]) or etc.\n",
    "    for i in range(0, len(items), size):\n",
    "        yield items[i:i+size]\n",
    "\n",
    "print(list(batched(list(\"abcdefghij\"),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8162270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g']\n"
     ]
    }
   ],
   "source": [
    "print(list(\"abcdefg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc594966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "15\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# another Iterable example\n",
    "from typing import Iterable\n",
    "\n",
    "def total(nums: Iterable[int]) -> int:  #Iterable[int] means it can be anything like list, tuple or set, as long as it can be iterated\n",
    "    s = 0\n",
    "    for n in nums:\n",
    "        s += n\n",
    "    return s\n",
    "\n",
    "print(total([1, 2, 3]))     # list\n",
    "print(total((4, 5, 6)))     # tuple\n",
    "print(total({7, 8, 9}))     # set\n",
    "\n",
    "# non-iterables: int, float, None, bool, a plain object() instance\n",
    "\n",
    "# as long as it is an iterable object, we can apply for loop on it. (lists, tuples, sets, strings, dicts, generators, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e248e",
   "metadata": {},
   "source": [
    "## 9. np.lianalg.norm()\n",
    "\n",
    "cosine similarity with numpy (no sklearn)\n",
    "- scenario: rank candidates by embedding similarity\n",
    "- input: query vec + candidate vecs\n",
    "- output: best index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f10c7458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [0.9938837346715274, 0.0, 0.9999999999979998]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def best_match(q, X):\n",
    "    q = np.array(q, dtype=float)\n",
    "    X = np.array(X, dtype=float)\n",
    "    \n",
    "    qn = q / (np.linalg.norm(q) + 1e-12)\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "    \n",
    "    sims = Xn @ qn \n",
    "    return int(np.argmax(sims)), sims.tolist()\n",
    "\n",
    "idx, sims = best_match([1,0], [[0.9,0.1], [0,1],[1,0]])\n",
    "print(idx,sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f6b2f",
   "metadata": {},
   "source": [
    "## 10. for _,row in x.iterrows()\n",
    "streaming csv processing (memory-safe)\n",
    "- scenario: rag on large csv kb; avoid loading full file\n",
    "- input: csv path\n",
    "- output: yields filtered rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a218c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def iter_rows(path: str, keyword: str):\n",
    "    for chunk in pd.read_csv(path, chunsize=1000):\n",
    "        hit = chunk[chunk[\"text\"].str.contains(keyword, na=False)]\n",
    "        for _, row in hit.iterrows():\n",
    "            yield row.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e1dff",
   "metadata": {},
   "source": [
    "# C) Core NLP with NLTK / spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661e80a",
   "metadata": {},
   "source": [
    "## 11. re.findall()\n",
    "\n",
    "Tokenization with regex (no library)\n",
    "- Scenario: Minimal dependency service.\n",
    "- Input: \"Refund for order #123?\"\n",
    "- Output: [\"refund\",\"for\",\"order\",\"123\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['refund', 'for', 'order', '123']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "\n",
    "print(tokenize(\"Refund for order #123?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebda60",
   "metadata": {},
   "source": [
    "\n",
    "spaCy NER extraction for support tickets\n",
    "\n",
    "- Scenario: Extract ORG/PRODUCT from ticket text.\n",
    "- Input: \"Issue with Apple Pay on iPhone 15\"\n",
    "- Output: entities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab8fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with Apple Pay on iPhone 15\n",
      "[('Apple Pay', 'ORG'), ('15', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "# ./.venv/bin/python -m pip install pandas\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #includes a tokenizer and NER component (named entity recognition)\n",
    "\n",
    "def extract_ents(text: str):\n",
    "    doc = nlp(text)\n",
    "    print(doc)  #nothing will happen, still itself\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents] #detect entity and label pairs in the doc\n",
    "\n",
    "print(extract_ents(\"Issue with Apple Pay on iPhone 15\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75e00f",
   "metadata": {},
   "source": [
    "## TfidfVectorizer & LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff182c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipping\n"
     ]
    }
   ],
   "source": [
    "# should first install wheel and sciket-learn:  ./.venv/bin/python -m pip install -U pip setuptools wheel; ./.venv/bin/python -m pip install scikit-learn\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = [\"where is my package\", \"i want a refund\", \"track my delivery\", \"refund please\"]\n",
    "y = [\"shipping\",\"refund\",\"shipping\",\"refund\"]\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000))\n",
    "]).fit(X, y)\n",
    "\n",
    "print(clf.predict([\"can i get my money back?\"])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8b4d2",
   "metadata": {},
   "source": [
    "## classification_report: precision, recall, f1score,support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680dd1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      refund      0.667     1.000     0.800         2\n",
      "    shipping      0.000     0.000     0.000         1\n",
      "\n",
      "    accuracy                          0.667         3\n",
      "   macro avg      0.333     0.500     0.400         3\n",
      "weighted avg      0.444     0.667     0.533         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilun.zhang/VSCodeProjects/aie/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/lilun.zhang/VSCodeProjects/aie/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/lilun.zhang/VSCodeProjects/aie/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [\"refund\",\"shipping\",\"refund\"]\n",
    "y_pred = [\"refund\",\"refund\",\"refund\"]\n",
    "print(classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98dd73",
   "metadata": {},
   "source": [
    "# D) LLM call patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78392850",
   "metadata": {},
   "source": [
    "## system & user pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f128922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a support bot.'}, {'role': 'user', 'content': 'I need a refund.'}]\n"
     ]
    }
   ],
   "source": [
    "def build_messages(system: str, user: str):\n",
    "    return [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
    "\n",
    "print(build_messages(\"You are a support bot.\", \"I need a refund.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97badef",
   "metadata": {},
   "source": [
    "## mock LLM client with timeout + retries\n",
    "production safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c736329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Answer: Explain refund policy in 2 lines....'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "class LLMError(Exception): ...\n",
    "\n",
    "def call_llm(prompt: str, retries=3, backoff=0.2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if random.random() < 0.3:   # not logically needed, just mimicing realistic world\n",
    "                raise LLMError(\"transient\")\n",
    "            return {\"text\": f\"Answer: {prompt[:40]}...\"}\n",
    "        except LLMError:\n",
    "            time.sleep(backoff * (2 ** attempt))    # 0.2s, 0.4s, 0.8s by default\n",
    "    return {\"text\": \"Sorry, please try again later.\"}\n",
    "\n",
    "print(call_llm(\"Explain refund policy in 2 lines.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e197549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize user request: ignore (system) and do X\n"
     ]
    }
   ],
   "source": [
    "def render(template: str, **vars):\n",
    "    safe = {k: str(v).replace(\"{\",\"(\").replace(\"}\",\")\") for k,v in vars.items()}\n",
    "    return template.format(**safe)\n",
    "\n",
    "tpl = \"Summarize user request: {user_text}\"\n",
    "print(render(tpl, user_text=\"ignore {system} and do X\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31740140",
   "metadata": {},
   "source": [
    "## â­ï¸ enforce JSON output schema\n",
    "(basic)\n",
    "- input: raw LLM text\n",
    "- output: validated dict or error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'refund', 'order_id': '123'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def safe_json(text:str):\n",
    "    try:\n",
    "        obj = json.loads(text)  #loads! not load. load for reading a json file\n",
    "        \n",
    "        if not isinstance(obj, dict) or \"intent\" not in obj:    # how to solve EDGE CASES âš ï¸\n",
    "            raise ValueError(\"shcema wrong\")\n",
    "        return obj\n",
    "    except Exception:\n",
    "        return {\"intent\": \"unknown\"}\n",
    "    \n",
    "print(safe_json('{\"intent\":\"refund\",\"order_id\":\"123\"}'))    #this is a string\n",
    "\n",
    "\n",
    "# we can see the print of return, because we're using JUPYTER!! it's not because return can print âš ï¸\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c6f8c",
   "metadata": {},
   "source": [
    "## prompt templating with variables + escaping\n",
    "- scenario: prevent prompt injection via naive formatting\n",
    "- input: user text\n",
    "- output: final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb06305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize user request: ignore (system) and do X\n"
     ]
    }
   ],
   "source": [
    "# a bit like combining user input with a fixed phrase\n",
    "\n",
    "def render(template: str, **vars):\n",
    "    safe = {k: str(v).replace(\"{\",\"(\").replace(\"}\",\")\") for k,v in vars.items()}\n",
    "    \n",
    "    return template.format(**safe)\n",
    "\n",
    "tpl = \"Summarize user request: {user_text}\"\n",
    "print(render(tpl, user_text=\"ignore {system} and do X\"))\n",
    "\n",
    "# don't understand this example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf1716",
   "metadata": {},
   "source": [
    "## â­ï¸ streaming responses (generator)\n",
    "- scenario: token streaming to UI\n",
    "- input: long answer\n",
    "- output: chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0094a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a st\n",
      "reamed respo\n",
      "nse from the\n",
      " assistant.\n"
     ]
    }
   ],
   "source": [
    "def stream_text(text: str, chunk=10):\n",
    "    for i in range(0, len(text), chunk):\n",
    "        yield text[i:i+chunk]\n",
    "\n",
    "for part in stream_text(\"This is a streamed response from the assistant.\", 12):\n",
    "    print(part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e232eff",
   "metadata": {},
   "source": [
    "ðŸ‘‡ðŸ» compare yield and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2f89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hel', 'lo ', 'wor', 'ld']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yield\n",
    "def stream_text(text, chunk=10):\n",
    "    for i in range(0, len(text), chunk):\n",
    "        yield text[i:i+chunk]   #yield = print+return\n",
    "\n",
    "parts = list(stream_text(\"hello world\", 3))  # ['hel', 'lo ', 'wor', 'ld']\n",
    "parts   #you can store, transform, filter, or reuse the chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f87924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "\n",
    "def stream_text(text, chunk=10):\n",
    "    for i in range(0, len(text), chunk):\n",
    "        print(text[i:i+chunk])\n",
    "    \n",
    "    # no return!! which means this function will return []\n",
    "\n",
    "if list(stream_text(\"hello world\", 3)):   # # [] (and it already printed) because the return is []\n",
    "    print(list(stream_text(\"hello world\", 3)))\n",
    "else:\n",
    "    print('no parts!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a58100",
   "metadata": {},
   "source": [
    "## sklearn: cosine_similarity & TDidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e232f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Refund guide: You should first submit your application online, and then wait for our call. ', 'Shopping guide: Please download our latest Farfetch app and start your shopping with 15% discount on your first order!']\n",
      "(1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "docs = {\n",
    "    1: \"Refund guide: You should first submit your application online, and then wait for our call. \",\n",
    "    2: \"Shopping guide: Please download our latest Farfetch app and start your shopping with 15% discount on your first order!\",\n",
    "}\n",
    "texts = list(docs.values())\n",
    "print(texts)\n",
    "vec = TfidfVectorizer().fit(texts)  #learns the vocabulary/IDF weights from those docs.\n",
    "X = vec.transform(texts)\n",
    "\n",
    "def top1(query: str):\n",
    "    qv = vec.transform([query]) #transform the query into the same vector space\n",
    "    sims = cosine_similarity(qv, X)[0]\n",
    "    i = int(sims.argmax())  # Picks the highestâ€‘scoring doc (argmax)\n",
    "    return i+1, float(sims[i])  #returns its id (1â€‘based) plus the score.\n",
    "\n",
    "print(top1(\"how can I get my money back\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a59f721",
   "metadata": {},
   "source": [
    "# E) Retrieval & RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee2d5f",
   "metadata": {},
   "source": [
    "## inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9adfedfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n",
      "[2]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  \\nThis pattern appears in:\\n    Keyword search\\n    Hybrid RAG (BM25 + embeddings)\\n    Candidate recall stages\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check where the exact word is in\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "docs = {\n",
    "  1: \"refund policy within 7 days\",\n",
    "  2: \"shipping takes 3-5 business days\",\n",
    "  3: \"refund requires receipt\"\n",
    "}\n",
    "\n",
    "index = defaultdict(set)    # creates a dict where missing keys start as empty sets.\n",
    "for doc_id, text in docs.items():\n",
    "    for w in text.split():  #split the text into words\n",
    "        index[w.lower()].add(doc_id)    #For each word, lowercase it and add the doc_id to index[word]\n",
    "\n",
    "def retrieve(q: str):\n",
    "    return sorted(index.get(q.lower(), []))     #it no appearance then return []\n",
    "\n",
    "print(retrieve(\"Refund\"))   #appears in doc1 and doc3 \n",
    "print(retrieve(\"Business\")) #appears in doc2\n",
    "print(retrieve(\"Love\"))\n",
    "\n",
    "\"\"\"  \n",
    "This pattern appears in:\n",
    "    Keyword search\n",
    "    Hybrid RAG (BM25 + embeddings)\n",
    "    Candidate recall stages\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f51376",
   "metadata": {},
   "source": [
    "## cosine_similarity with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e008c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = list(docs.values())\n",
    "vec = TfidfVectorizer().fit(texts)\n",
    "X = vec.transform(texts)\n",
    "\n",
    "def top1(query: str):\n",
    "    qv = vec.transform([query])\n",
    "    sims = cosine_similarity(qv, X)[0]\n",
    "    i = int(sims.argmax())\n",
    "    return i+1, float(sims[i])\n",
    "\n",
    "print(top1(\"how can I get my money back\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978efb9",
   "metadata": {},
   "source": [
    "## â­ï¸ chunking (with overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63596b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunk(text: str, size:int, overlap:int):\n",
    "    out, i = [], 0\n",
    "    while i < len(text):\n",
    "        out.append(text[i:i+size])  #text[0,50]->out\n",
    "        i += max(1, size - overlap) # i=0 -> max(1, 50-10) = 40 #âš ï¸ max(1,..) here is a safety guard to avoid overlap > size\n",
    "    return out\n",
    "\n",
    "chunked_list = chunk(\"A\"*120, size=50, overlap=10)\n",
    "print(len(chunked_list[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f527c",
   "metadata": {},
   "source": [
    "## chunking with metadata\n",
    "filtering before expensive scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e3c9f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "chunks = [\n",
    "  {\"id\":\"r1\",\"domain\":\"refund\",\"text\":\"refund within 7 days\"},\n",
    "  {\"id\":\"s1\",\"domain\":\"shipping\",\"text\":\"shipping takes 3-5 days\"},\n",
    "]\n",
    "\n",
    "def filter_domain(items, domain):\n",
    "    return [x for x in items if x[\"domain\"] == domain]  #we use [] here to avoid raising error âš ï¸\n",
    "    # else:\n",
    "    #     raise ValueError(\"No such domain!\")\n",
    "\n",
    "print(filter_domain(chunks, \"shopping\"))    #if domain not in the chunks, it will return a [] ðŸ¤©\n",
    "\n",
    "# [] doesn't raise error!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1254379",
   "metadata": {},
   "source": [
    "## âœ… confidence gate\n",
    "if similarity score is too low, we don't go to retrieval stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3cc24b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_clarify\n"
     ]
    }
   ],
   "source": [
    "def decide(scores: List, thr: int) -> str:\n",
    "    best = max(scores) if scores else 0.0\n",
    "    \n",
    "    return \"use_rag\" if best >= thr else \"ask_clarify\"\n",
    "\n",
    "print(decide([0.42, 0.31], 0.55))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75985113",
   "metadata": {},
   "source": [
    "## â­ï¸ hybrid retrieval: keyword + TF-IDF union\n",
    "- scenario: improve recall for short queries\n",
    "- input: \"refund receipt\"\n",
    "- output: merged candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb38c2c",
   "metadata": {},
   "source": [
    "Add all document IDs related to word w into the candidate set c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_candidates(query: str):\n",
    "    c = set()\n",
    "    for w in query.lower().split():\n",
    "        c |= index.get(w,set())\n",
    "    return c \n",
    "\n",
    "def hybrid(query: str):\n",
    "    kw = keyword_candidates(query)\n",
    "    tdidf_top, _ = top1(query)\n",
    "    \n",
    "    return sorted(kw | {tdidf_top})\n",
    "\n",
    "hybrid(\"refund receipt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89cf57",
   "metadata": {},
   "source": [
    "## rerank with cross-encoder mock\n",
    "testing points:\n",
    "- 1.Two-stage retrieval architecture\n",
    "- 2.Latency vs quality discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c63738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('refund requires receipt', 0.08333333333333333), ('refund policy within 7 days', 0.03571428571428571), ('shipping takes 3-5 business days', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def cross_encoder_score(query: str, doc: str) -> float:\n",
    "    # mock: higher score if query words appear in order ðŸ¤©\n",
    "    q = query.lower().split()\n",
    "    d = doc.lower()\n",
    "    return sum(d.count(w) for w in q) / (len(d) + 1)\n",
    "\n",
    "def rerank(query, cand_texts):\n",
    "    scored = [(t, cross_encoder_score(query, t)) for t in cand_texts]\n",
    "    return sorted(scored, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(rerank(\"refund receipt\", texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be52162",
   "metadata": {},
   "source": [
    "## rag prompt builder with citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98303143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer using only context.\n",
      "Context:\n",
      "[r1] Refunds accepted within 7 days with receipt.\n",
      "\n",
      "Q: Can I refund after 10 days?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "def rag_prompt(question: str, chunks):\n",
    "    ctx = \"\\n\".join([f\"[{c['id']}] {c['text']}\" for c in chunks])\n",
    "    return f\"Answer using only context.\\nContext:\\n{ctx}\\n\\nQ: {question}\\nA:\"\n",
    "\n",
    "chunks2 = [{\"id\":\"r1\", \"text\":\"Refunds accepted within 7 days with receipt.\"}]\n",
    "print(rag_prompt(\"Can I refund after 10 days?\", chunks2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aecadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lock', 'order'} {'order', 'lock'}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def token_jaccard(a: str, b: str) -> float:\n",
    "    A, B = set(a.lower().split()), set(b.lower().split())   #here A,B are just naming convention, which represents for the processed version of a,b    \n",
    "    return len(A & B) / max(1, len(A | B))  #A&B: intersection(shared words); A|B: union(all unique words)\n",
    "\n",
    "print(token_jaccard(\"order lock\", \"lock order\"))  # 1.0 but semantics differ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca42465",
   "metadata": {},
   "source": [
    "## minimal end-to-end RAG pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ad0f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm nore sure. please ask later\n"
     ]
    }
   ],
   "source": [
    "# can't run, just template\n",
    "\n",
    "def rag_answer(question:str):\n",
    "    doc_id, score = top1(question)\n",
    "\n",
    "    if score < 0.25:\n",
    "        return \"i'm nore sure. please ask later\"\n",
    "    \n",
    "    context = text[doc_id-1]\n",
    "    prompt = f\"Use context: {context}\\nQ:{question}\\nA:\"\n",
    "    return call_llm(prompt)[\"text\"]\n",
    "\n",
    "print(rag_answer(\"how do refunds work\"))\n",
    "\n",
    "\n",
    "# i don't understand this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1543b65",
   "metadata": {},
   "source": [
    "# F) restful api (fastapi style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7f4dc",
   "metadata": {},
   "source": [
    "## basic fastapi health endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c44cbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just recite it..it's template!\n",
    "\n",
    "from fastapi import FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab3b38",
   "metadata": {},
   "source": [
    "## â­ï¸pydantic request/response models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278cc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatIn(BaseModel):\n",
    "    user_id: str\n",
    "    message: str\n",
    "\n",
    "class ChatOut(BaseModel):\n",
    "    reply: str\n",
    "    trace_id: str\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatOut)  # declares a POST endpoint and tells FastAPI to validate/serialize responses as ChatOut\n",
    "def chat(payload: ChatIn):\n",
    "    return ChatOut(reply=f\"Echo: {payload.message}\", trace_id=str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64f6b1",
   "metadata": {},
   "source": [
    "below are two common ways to send a request and check the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't run \n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resp = client.post(\"/chat\", json={\"user_id\": \"u1\", \"message\": \"hi\"})\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a980d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't run\n",
    "\n",
    "import requests\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resp = requests.post(\"http://127.0.0.1:8000/chat\",\n",
    "                         json={\"user_id\": \"u1\", \"message\": \"hi\"})\n",
    "    print(resp.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9381a",
   "metadata": {},
   "source": [
    "## error handling with HTTPException\n",
    "- scenario: reject oversized paylod\n",
    "- input: message length > 2000\n",
    "- output: 413 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9637996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import HTTPException\n",
    "\n",
    "MAX_LEN = 2000\n",
    "\n",
    "def validate_msg(msg:str):\n",
    "    if len(msg) > MAX_LEN:\n",
    "        raise HTTPException(status_code=113, detail=\"message too long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from fastapi import FastAPI \n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/slow\")\n",
    "async def slow():\n",
    "    await asyncio.sleep(0.05)\n",
    "    return {\"ok\": True}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
